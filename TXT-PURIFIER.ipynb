{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TXT PURIFIER\n",
    "\n",
    "This notebook was posted by Simon Lindgren // [@simonlindgren](http://www.twitter.com/simonlindgren) // [simonlindgren.com](http://simonlindgren.com)\n",
    "\n",
    "The following code is about how purify a list of documents by removing their most common words to reveal the words that truly distinguish the documents from each other.\n",
    "\n",
    "Required Python packages are `gensim` and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"docs.txt\", 'r') # a txt file with one document per line\n",
    "documents = f.readlines() # documents is now a Python list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(documents) # number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(documents[0][:100]) # inspect the beginning of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word removal etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words, tokenize, and convert to lowercase\n",
    "stoplist = set('your stop words here'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove project-specific stop words\n",
    "stoplist2 = set('another set of stop words here'.split())\n",
    "texts = [[word for word in text if word not in stoplist2] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that appear less than X (e.g. 2) time(s)\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove anything which is not pure letters\n",
    "# the method isalpha() checks whether the string consists of alphabetic characters only\n",
    "\n",
    "texts = [[token for token in text if token.isalpha()] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove one-letter words\n",
    "texts = [[token for token in text if len(token) > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(texts[0][:40]) # see the beginning of the first tokenized and cleaned document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we now create a gensim corpus from this set of documents\n",
    "# to be able to get tf-idf scores for words\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "tfidf = models.TfidfModel(corpus, id2word = dictionary)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "low_value = 0.25\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "\n",
    "d = {dictionary.get(id): value for doc in corpus_tfidf for id, value in doc} # a dictionary of the tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the dictionary into a Pandas dataframe and sort descending based on tf-idf\n",
    "df = pd.DataFrame([[key,value] for key,value in d.items()],columns=[\"word\",\"tf-idf\"])\n",
    "df = df.sort_values(['tf-idf'], ascending=[False])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_threshold = 0.058 # set manually (experiment and iterate)\n",
    "\n",
    "df2 = df.loc[df['tf-idf'] > tfidf_threshold]\n",
    "print(str(len(df2)) + ' are left of ' + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the word column from df2 as a list\n",
    "# this is a list of all words with tf-idf above the threshold\n",
    "\n",
    "keep_words = df2['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the low tf-idf words that are not to be kept\n",
    "\n",
    "texts = [[word for word in text if word in keep_words] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'texts' is now a list of lists of tokens\n",
    "# we transform it back to the initial format (a list of documents)\n",
    "\n",
    "doc_list = [] # initialise an empty list\n",
    "\n",
    "for token_list in texts:\n",
    "    #print(token_list)\n",
    "    token_string = ' '.join(token_list)\n",
    "    #print(token_string)\n",
    "    #print(\"==========\")\n",
    "    doc_list.append(token_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove duplicates, if any, from the doc_list\n",
    "\n",
    "doc_list2= set(doc_list)\n",
    "print(str(len(doc_list)-len(doc_list2)) + \" duplicate documents removed.\")\n",
    "doc_list = doc_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many documents are left?\n",
    "len(doc_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write the documents as lines to a new txt file\n",
    "with open('docs.txt', 'w') as outfile:\n",
    "    for item in doc_list:\n",
    "        outfile.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
